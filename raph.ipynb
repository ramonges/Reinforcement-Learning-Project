{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/qtyfs34j79n27bt7y8vw2n140000gn/T/ipykernel_979/2509170913.py:2: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  gaz['Date']=pd.to_datetime(gaz['Date'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Dernier', 'Ouv.', ' Plus Haut', 'Plus Bas', 'Vol.',\n",
       "       'Variation %'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz = pd.read_csv('/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/Dutch TTF Natural Gas Futures - Données Historiques (1).csv')\n",
    "gaz['Date']=pd.to_datetime(gaz['Date'])\n",
    "lists = ['Dernier', 'Ouv.', ' Plus Haut', 'Plus Bas', 'Vol.','Variation %']\n",
    "for i in lists: \n",
    "    gaz[i] = gaz[i].str.replace(',', '.')\n",
    "gaz.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Dernier</th>\n",
       "      <th>Ouv.</th>\n",
       "      <th>Plus Haut</th>\n",
       "      <th>Plus Bas</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Variation %</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>ema20</th>\n",
       "      <th>ema50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-15</td>\n",
       "      <td>27.050</td>\n",
       "      <td>26.255</td>\n",
       "      <td>27.085</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0.08K</td>\n",
       "      <td>3.84%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-14</td>\n",
       "      <td>26.050</td>\n",
       "      <td>24.675</td>\n",
       "      <td>26.600</td>\n",
       "      <td>24.675</td>\n",
       "      <td>0.51K</td>\n",
       "      <td>4.60%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-13</td>\n",
       "      <td>24.905</td>\n",
       "      <td>25.100</td>\n",
       "      <td>25.425</td>\n",
       "      <td>24.555</td>\n",
       "      <td>0.24K</td>\n",
       "      <td>0.52%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>24.775</td>\n",
       "      <td>25.010</td>\n",
       "      <td>25.010</td>\n",
       "      <td>24.355</td>\n",
       "      <td>0.30K</td>\n",
       "      <td>-0.62%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-11</td>\n",
       "      <td>24.930</td>\n",
       "      <td>25.970</td>\n",
       "      <td>26.465</td>\n",
       "      <td>24.760</td>\n",
       "      <td>0.45K</td>\n",
       "      <td>-5.53%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.44%</td>\n",
       "      <td>34.299402</td>\n",
       "      <td>-0.128800</td>\n",
       "      <td>19.074238</td>\n",
       "      <td>19.420546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>2017-10-26</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.22%</td>\n",
       "      <td>33.531057</td>\n",
       "      <td>-0.117551</td>\n",
       "      <td>18.978597</td>\n",
       "      <td>19.367584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84%</td>\n",
       "      <td>34.323259</td>\n",
       "      <td>-0.098296</td>\n",
       "      <td>18.895873</td>\n",
       "      <td>19.318267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.72%</td>\n",
       "      <td>32.747079</td>\n",
       "      <td>-0.086934</td>\n",
       "      <td>18.806742</td>\n",
       "      <td>19.265001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-78.97%</td>\n",
       "      <td>35.511086</td>\n",
       "      <td>-0.063048</td>\n",
       "      <td>18.738481</td>\n",
       "      <td>19.218923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1617 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Dernier    Ouv.  Plus Haut Plus Bas   Vol. Variation %  \\\n",
       "0    2024-03-15   27.050  26.255     27.085   26.000  0.08K       3.84%   \n",
       "1    2024-03-14   26.050  24.675     26.600   24.675  0.51K       4.60%   \n",
       "2    2024-03-13   24.905  25.100     25.425   24.555  0.24K       0.52%   \n",
       "3    2024-03-12   24.775  25.010     25.010   24.355  0.30K      -0.62%   \n",
       "4    2024-03-11   24.930  25.970     26.465   24.760  0.45K      -5.53%   \n",
       "...         ...      ...     ...        ...      ...    ...         ...   \n",
       "1612 2017-10-27   18.150  18.150     18.150   18.150    NaN       0.44%   \n",
       "1613 2017-10-26   18.070  18.070     18.070   18.070    NaN      -0.22%   \n",
       "1614 2017-10-25   18.110  18.110     18.110   18.110    NaN       0.84%   \n",
       "1615 2017-10-24   17.960  17.960     17.960   17.960    NaN      -0.72%   \n",
       "1616 2017-10-23   18.090  18.090     18.090   18.090    NaN     -78.97%   \n",
       "\n",
       "            rsi      macd      ema20      ema50  \n",
       "0           NaN       NaN        NaN        NaN  \n",
       "1           NaN       NaN        NaN        NaN  \n",
       "2           NaN       NaN        NaN        NaN  \n",
       "3           NaN       NaN        NaN        NaN  \n",
       "4           NaN       NaN        NaN        NaN  \n",
       "...         ...       ...        ...        ...  \n",
       "1612  34.299402 -0.128800  19.074238  19.420546  \n",
       "1613  33.531057 -0.117551  18.978597  19.367584  \n",
       "1614  34.323259 -0.098296  18.895873  19.318267  \n",
       "1615  32.747079 -0.086934  18.806742  19.265001  \n",
       "1616  35.511086 -0.063048  18.738481  19.218923  \n",
       "\n",
       "[1617 rows x 11 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz['Dernier'] = pd.to_numeric(gaz['Dernier'], errors='coerce')\n",
    "\n",
    "gaz['rsi'] = ta.momentum.rsi(gaz['Dernier'], window=14)\n",
    "gaz['macd'] = ta.trend.macd_diff(gaz['Dernier'])\n",
    "gaz['ema20'] = ta.trend.ema_indicator(gaz['Dernier'], window=20)\n",
    "gaz['ema50'] = ta.trend.ema_indicator(gaz['Dernier'], window=50)\n",
    "\n",
    "gaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_k_m_to_numeric(value):\n",
    "    \"\"\"\n",
    "    Convert values with 'K' or 'M' suffix to float numbers.\n",
    "    Args:\n",
    "    - value: The string or numeric value to convert.\n",
    "    \n",
    "    Returns:\n",
    "    - The converted value as float if 'K' or 'M' was found; otherwise, the original value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):  # Only process strings\n",
    "        if value.endswith('K'):\n",
    "            return float(value[:-1]) * 1e3\n",
    "        elif value.endswith('M'):\n",
    "            return float(value[:-1]) * 1e6\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz['Vol.'] = gaz['Vol.'].apply(convert_k_m_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in gaz.columns:\n",
    "    gaz[column] = pd.to_numeric(gaz[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/qtyfs34j79n27bt7y8vw2n140000gn/T/ipykernel_979/2367427009.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  assert gaz.applymap(np.isreal).all().all(), \"Non-numeric data found in the dataset.\"\n"
     ]
    }
   ],
   "source": [
    "assert gaz.applymap(np.isreal).all().all(), \"Non-numeric data found in the dataset.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def calculate_technical_indicators(self):\n",
    "        \"\"\"\n",
    "        Calculate specified technical indicators for the dataset.\n",
    "        \"\"\"\n",
    "        self.data['rsi'] = ta.momentum.rsi(self.data['Dernier'], window=14)\n",
    "        self.data['macd'] = ta.trend.macd_diff(self.data['Dernier'])\n",
    "        self.data['ema20'] = ta.trend.ema_indicator(self.data['Dernier'], window=20)\n",
    "        self.data['ema50'] = ta.trend.ema_indicator(self.data['Dernier'], window=50)\n",
    "\n",
    "    def normalize_features(self):\n",
    "        \"\"\"\n",
    "        Normalize features to have a similar scale.\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        features = ['rsi', 'macd', 'ema20', 'ema50']  # Specify the features to normalize\n",
    "        self.data[features] = scaler.fit_transform(self.data[features])\n",
    "\n",
    "    def integrate_economic_indicators(self, economic_data):\n",
    "        \"\"\"\n",
    "        Integrate economic indicators with market data.\n",
    "        This function assumes economic_data is a DataFrame where columns are indicators and rows align with self.data's timeline.\n",
    "        \"\"\"\n",
    "        self.data = pd.concat([self.data, economic_data], axis=1)\n",
    "\n",
    "    def construct_feature_vector(self):\n",
    "        \"\"\"\n",
    "        Combine all features into a single vector for each timestep.\n",
    "        Assuming all necessary features are already columns in self.data,\n",
    "        this function will return a numpy array representation of the DataFrame.\n",
    "        \"\"\"\n",
    "        return self.data.values\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'gaz' is your DataFrame loaded with 'Dernier' prices and indexed by datetime\n",
    "feature_engineer = FeatureEngineer(gaz)\n",
    "\n",
    "feature_engineer.calculate_technical_indicators()  # Calculate RSI, MACD, EMA20, EMA50\n",
    "# If you have economic data to integrate, load it and pass it to the integrate_economic_indicators method\n",
    "# economic_data = pd.read_csv('path_to_your_economic_data.csv', index_col='date', parse_dates=True)\n",
    "# feature_engineer.integrate_economic_indicators(economic_data)\n",
    "\n",
    "feature_engineer.normalize_features()  # Normalize the features\n",
    "features_vector = feature_engineer.construct_feature_vector()  # Get the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Dernier</th>\n",
       "      <th>Ouv.</th>\n",
       "      <th>Plus Haut</th>\n",
       "      <th>Plus Bas</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Variation %</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>ema20</th>\n",
       "      <th>ema50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1710460800000000000</td>\n",
       "      <td>27.050</td>\n",
       "      <td>26.255</td>\n",
       "      <td>27.085</td>\n",
       "      <td>26.000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1710374400000000000</td>\n",
       "      <td>26.050</td>\n",
       "      <td>24.675</td>\n",
       "      <td>26.600</td>\n",
       "      <td>24.675</td>\n",
       "      <td>510.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1710288000000000000</td>\n",
       "      <td>24.905</td>\n",
       "      <td>25.100</td>\n",
       "      <td>25.425</td>\n",
       "      <td>24.555</td>\n",
       "      <td>240.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1710201600000000000</td>\n",
       "      <td>24.775</td>\n",
       "      <td>25.010</td>\n",
       "      <td>25.010</td>\n",
       "      <td>24.355</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1710115200000000000</td>\n",
       "      <td>24.930</td>\n",
       "      <td>25.970</td>\n",
       "      <td>26.465</td>\n",
       "      <td>24.760</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>1509062400000000000</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.224810</td>\n",
       "      <td>0.475943</td>\n",
       "      <td>0.059504</td>\n",
       "      <td>0.069041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>1508976000000000000</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213397</td>\n",
       "      <td>0.476425</td>\n",
       "      <td>0.059111</td>\n",
       "      <td>0.068779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>1508889600000000000</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.225165</td>\n",
       "      <td>0.477251</td>\n",
       "      <td>0.058771</td>\n",
       "      <td>0.068534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>1508803200000000000</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201751</td>\n",
       "      <td>0.477738</td>\n",
       "      <td>0.058404</td>\n",
       "      <td>0.068270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>1508716800000000000</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.242810</td>\n",
       "      <td>0.478763</td>\n",
       "      <td>0.058123</td>\n",
       "      <td>0.068042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1617 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date  Dernier    Ouv.   Plus Haut  Plus Bas   Vol.  \\\n",
       "0     1710460800000000000   27.050  26.255      27.085    26.000   80.0   \n",
       "1     1710374400000000000   26.050  24.675      26.600    24.675  510.0   \n",
       "2     1710288000000000000   24.905  25.100      25.425    24.555  240.0   \n",
       "3     1710201600000000000   24.775  25.010      25.010    24.355  300.0   \n",
       "4     1710115200000000000   24.930  25.970      26.465    24.760  450.0   \n",
       "...                   ...      ...     ...         ...       ...    ...   \n",
       "1612  1509062400000000000   18.150  18.150      18.150    18.150    NaN   \n",
       "1613  1508976000000000000   18.070  18.070      18.070    18.070    NaN   \n",
       "1614  1508889600000000000   18.110  18.110      18.110    18.110    NaN   \n",
       "1615  1508803200000000000   17.960  17.960      17.960    17.960    NaN   \n",
       "1616  1508716800000000000   18.090  18.090      18.090    18.090    NaN   \n",
       "\n",
       "      Variation %       rsi      macd     ema20     ema50  \n",
       "0             NaN       NaN       NaN       NaN       NaN  \n",
       "1             NaN       NaN       NaN       NaN       NaN  \n",
       "2             NaN       NaN       NaN       NaN       NaN  \n",
       "3             NaN       NaN       NaN       NaN       NaN  \n",
       "4             NaN       NaN       NaN       NaN       NaN  \n",
       "...           ...       ...       ...       ...       ...  \n",
       "1612          NaN  0.224810  0.475943  0.059504  0.069041  \n",
       "1613          NaN  0.213397  0.476425  0.059111  0.068779  \n",
       "1614          NaN  0.225165  0.477251  0.058771  0.068534  \n",
       "1615          NaN  0.201751  0.477738  0.058404  0.068270  \n",
       "1616          NaN  0.242810  0.478763  0.058123  0.068042  \n",
       "\n",
       "[1617 rows x 11 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GYM Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=10000, transaction_cost=0.001):\n",
    "        self.data = data\n",
    "        self.state_space = data.shape[1]  # This should match the number of features used to represent a state\n",
    "        self.action_space = 3  # For example: buy, sell,\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.done = False\n",
    "        self.position = 0\n",
    "        self.history = []  # To store trade history\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return self.data.iloc[self.current_step]\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate taking an action and returning the next state, reward, and done status\n",
    "        \n",
    "        # Ensure action is within a valid range\n",
    "        action = np.clip(action, -1, 1)\n",
    "\n",
    "        # Calculate the number of shares bought/sold based on the action\n",
    "        delta_position = action * self.balance  # This assumes all-in on each action\n",
    "\n",
    "        # Get the current price from the dataset to calculate changes in portfolio value\n",
    "        current_price = self.data.iloc[self.current_step]['Dernier']  # Assuming 'close' is a column in your dataset\n",
    "        next_step = min(self.current_step + 1, len(self.data) - 1)  # Ensure we don't go past the end of the dataset\n",
    "        next_price = self.data.iloc[next_step]['Dernier']\n",
    "\n",
    "        # Update position and balance\n",
    "        change_in_value = delta_position * (next_price - current_price) / current_price\n",
    "        self.balance += change_in_value - (abs(delta_position) * self.transaction_cost)\n",
    "        self.portfolio_value = self.balance  # This could be more complex if managing multiple positions\n",
    "        self.position += delta_position\n",
    "\n",
    "        self.current_step = next_step\n",
    "\n",
    "        # Check if we're at the end\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        # Here, the reward could be the change in portfolio value, or some other metric\n",
    "        reward = change_in_value - (abs(delta_position) * self.transaction_cost)\n",
    "        \n",
    "        # Record this step\n",
    "        self.history.append((self.current_step, self.position, self.portfolio_value, reward))\n",
    "\n",
    "        return self._next_observation(), reward, self.done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Step:\", self.current_step)\n",
    "        print(\"Balance:\", self.balance)\n",
    "        print(\"Position:\", self.position)\n",
    "        print(\"Portfolio Value:\", self.portfolio_value)\n",
    "\n",
    "    def run_backtest(self, policy):\n",
    "        \"\"\"\n",
    "        Run a full backtest of the trading environment using the provided policy.\n",
    "        The policy function should take a state as input and return an action.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        while not self.done:\n",
    "            current_state = self._next_observation()\n",
    "            action = policy(current_state)\n",
    "            self.step(action)\n",
    "        return self.history\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Define a simple policy function (replace this with your PPO agent's policy for real testing)\n",
    "def sample_policy(state):\n",
    "    # This is a dummy policy that randomly decides to buy, hold, or sell\n",
    "    return np.random.uniform(-1, 1)\n",
    "\n",
    "# Assuming 'gaz_data' is your preprocessed DataFrame including 'Dernier' prices and any features\n",
    "env = TradingEnvironment(gaz)\n",
    "backtest_history = env.run_backtest(sample_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * next_value * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        next_value = values[step]\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return torch.tensor(returns)\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dims, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.mean_layer = nn.Linear(64, n_actions)\n",
    "        self.variance_layer = nn.Linear(64, n_actions)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.mean_layer(x)\n",
    "        variance = torch.exp(self.variance_layer(x))  # Ensure variance is positive\n",
    "        return mean, variance  \n",
    "\n",
    "def update_policy(self, states, actions, rewards, next_states, dones):\n",
    "    # Calculate advantages and discounted rewards\n",
    "    # Note: This will require implementing or calling additional methods to calculate these values\n",
    "    advantages, discounted_rewards = self.calculate_advantages(rewards, states, next_states, dones)\n",
    "    \n",
    "    # Convert lists to numpy arrays for processing\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    advantages = np.array(advantages)\n",
    "    discounted_rewards = np.array(discounted_rewards)\n",
    "\n",
    "    # Update actor\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Calculate loss for the actor\n",
    "        # Note: You'll need to implement the logic to calculate the actor's loss based on the current policy and advantages\n",
    "        actor_loss = self.calculate_actor_loss(states, actions, advantages)\n",
    "    actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "    self.actor.optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "\n",
    "    # Update critic\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Calculate loss for the critic\n",
    "        # Note: Implement logic to calculate the critic's loss based on the difference between discounted rewards and the critic's value predictions\n",
    "        critic_loss = self.calculate_critic_loss(states, discounted_rewards)\n",
    "    critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "    self.critic.optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_99\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_149 (InputLayer)      [(None, 11)]              0         \n",
      "                                                                 \n",
      " dense_297 (Dense)           (None, 64)                768       \n",
      "                                                                 \n",
      " dense_298 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_299 (Dense)           (None, 3)                 195       \n",
      "                                                                 \n",
      " tf.math.multiply_50 (TFOpL  (None, 3)                 0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5123 (20.01 KB)\n",
      "Trainable params: 5123 (20.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_100\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_150 (InputLayer)      [(None, 11)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_151 (InputLayer)      [(None, 3)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate_49 (Concatenat  (None, 14)                   0         ['input_150[0][0]',           \n",
      " e)                                                                  'input_151[0][0]']           \n",
      "                                                                                                  \n",
      " dense_300 (Dense)           (None, 64)                   960       ['concatenate_49[0][0]']      \n",
      "                                                                                                  \n",
      " dense_301 (Dense)           (None, 64)                   4160      ['dense_300[0][0]']           \n",
      "                                                                                                  \n",
      " dense_302 (Dense)           (None, 1)                    65        ['dense_301[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5185 (20.25 KB)\n",
      "Trainable params: 5185 (20.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import legacy as optimizers_legacy\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_size, action_size, action_bound):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_bound = action_bound\n",
    "        self.gamma = 0.99\n",
    "        self.lambda_ = 0.95\n",
    "        self.epsilon = 0.2\n",
    "        self.lr_actor = 1e-4\n",
    "        self.lr_critic = 2e-4\n",
    "        \n",
    "        # Build actor and critic models within the constructor\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "    def build_actor(self):\n",
    "        \"\"\"Builds the actor model.\"\"\"\n",
    "        state_input = Input(shape=(self.state_size,))\n",
    "        dense1 = Dense(64, activation='relu')(state_input)\n",
    "        dense2 = Dense(64, activation='relu')(dense1)\n",
    "        action_output = Dense(self.action_size, activation='tanh')(dense2)\n",
    "        action_output = self.action_bound * action_output\n",
    "        \n",
    "        model = Model(inputs=state_input, outputs=action_output)\n",
    "        model.compile(optimizer=optimizers_legacy.Adam(learning_rate=self.lr_actor))\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def build_critic(self):\n",
    "        \"\"\"Builds the critic model.\"\"\"\n",
    "        state_input = Input(shape=(self.state_size,))\n",
    "        action_input = Input(shape=(self.action_size,))\n",
    "        concat = Concatenate()([state_input, action_input])\n",
    "        \n",
    "        dense1 = Dense(64, activation='relu')(concat)\n",
    "        dense2 = Dense(64, activation='relu')(dense1)\n",
    "        q_value_output = Dense(1, activation='linear')(dense2)\n",
    "        \n",
    "        model = Model(inputs=[state_input, action_input], outputs=q_value_output)\n",
    "        model.compile(optimizer=optimizers_legacy.Adam(learning_rate=self.lr_critic))\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        # Convert Timestamps to float (Unix time) if present\n",
    "        if isinstance(state, pd.DataFrame) or isinstance(state, pd.Series):\n",
    "            state = state.apply(lambda x: x.timestamp() if isinstance(x, pd.Timestamp) else x)\n",
    "        elif isinstance(state, np.ndarray) and state.dtype == 'object':\n",
    "            state = np.array([x.timestamp() if isinstance(x, pd.Timestamp) else x for x in state], dtype=np.float32)\n",
    "        return state\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        # Preprocess the state to ensure numeric representation\n",
    "        state = self.preprocess_state(state)\n",
    "        \n",
    "        # Reshape the state to match the expected input dimensions of the model\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "                \n",
    "        # Predict action probabilities and choose the action with the highest probability\n",
    "        action_probabilities = self.actor.predict(state)\n",
    "        return np.argmax(action_probabilities)\n",
    "    \n",
    "    \n",
    "\n",
    "# Define the environment and PPO agent parameters\n",
    "state_size = 100  # Assume 100 features for the state\n",
    "action_size = 1  # Buy/sell quantity as a single action\n",
    "action_bound = 1  # Action limit (e.g., normalized quantity between -1 and 1)\n",
    "\n",
    "# Instantiate the PPOAgent\n",
    "agent = PPOAgent(state_size=env.state_space, action_size=env.action_space, action_bound=1)\n",
    "\n",
    "# Now you can access actor and critic through the agent object\n",
    "actor_model = agent.actor\n",
    "critic_model = agent.critic\n",
    "\n",
    "# Optionally, print summaries to verify the models\n",
    "actor_model.summary()\n",
    "critic_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.predict_action(state)  # Predict the action for the current state\n",
    "            next_state, reward, done, _ = env.step(action)  # Take the action in the environment\n",
    "            agent.update_policy(state, action, reward, next_state, done)  # Update the policy\n",
    "            state = next_state\n",
    "        print(f\"Episode {episode + 1}: Complete\")\n",
    "    print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_pnl = 0\n",
    "    print(\"Date\\t\\tAction\\tReward\\tPortfolio Value\")\n",
    "\n",
    "    while not done:\n",
    "        action = agent.predict_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Action interpretation for logging: -1 (Sell), 1 (Buy), 0 (Hold)\n",
    "        action_str = \"Buy\" if action > 0 else \"Sell\" if action < 0 else \"Hold\"\n",
    "\n",
    "        # Assuming 'date' is part of the environment's state\n",
    "        date = state.index[env.current_step].strftime('%Y-%m-%d')\n",
    "        print(f\"{date}\\t{action_str}\\t{reward:.2f}\\t{env.portfolio_value:.2f}\")\n",
    "\n",
    "        total_pnl += reward\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Final PnL: {total_pnl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PPOAgent' object has no attribute 'update_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb Cell 21\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m agent \u001b[39m=\u001b[39m PPOAgent(state_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mstate_space, action_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space, action_bound\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Adjust parameters as needed\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Proceed with training and running the simulation\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_agent(env, agent)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m run_simulation(env, agent)\n",
      "\u001b[1;32m/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb Cell 21\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mpredict_action(state)  \u001b[39m# Predict the action for the current state\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)  \u001b[39m# Take the action in the environment\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mupdate_policy(state, action, reward, next_state, done)  \u001b[39m# Update the policy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/raph.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{\u001b[39;00mepisode\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: Complete\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPOAgent' object has no attribute 'update_policy'"
     ]
    }
   ],
   "source": [
    "env = TradingEnvironment(gaz)  # Ensure 'gaz' is your DataFrame properly prepared\n",
    "agent = PPOAgent(state_size=env.state_space, action_size=env.action_space, action_bound=1)  # Adjust parameters as needed\n",
    "\n",
    "# Proceed with training and running the simulation\n",
    "train_agent(env, agent)\n",
    "run_simulation(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
