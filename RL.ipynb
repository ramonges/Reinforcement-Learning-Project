{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/qtyfs34j79n27bt7y8vw2n140000gn/T/ipykernel_2282/2509170913.py:2: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  gaz['Date']=pd.to_datetime(gaz['Date'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Dernier', 'Ouv.', ' Plus Haut', 'Plus Bas', 'Vol.',\n",
       "       'Variation %'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz = pd.read_csv('/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/Dutch TTF Natural Gas Futures - Données Historiques (1).csv')\n",
    "gaz['Date']=pd.to_datetime(gaz['Date'])\n",
    "lists = ['Dernier', 'Ouv.', ' Plus Haut', 'Plus Bas', 'Vol.','Variation %']\n",
    "for i in lists: \n",
    "    gaz[i] = gaz[i].str.replace(',', '.')\n",
    "gaz.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Dernier</th>\n",
       "      <th>Ouv.</th>\n",
       "      <th>Plus Haut</th>\n",
       "      <th>Plus Bas</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Variation %</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>ema20</th>\n",
       "      <th>ema50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-15</td>\n",
       "      <td>27.050</td>\n",
       "      <td>26.255</td>\n",
       "      <td>27.085</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0.08K</td>\n",
       "      <td>3.84%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-14</td>\n",
       "      <td>26.050</td>\n",
       "      <td>24.675</td>\n",
       "      <td>26.600</td>\n",
       "      <td>24.675</td>\n",
       "      <td>0.51K</td>\n",
       "      <td>4.60%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-13</td>\n",
       "      <td>24.905</td>\n",
       "      <td>25.100</td>\n",
       "      <td>25.425</td>\n",
       "      <td>24.555</td>\n",
       "      <td>0.24K</td>\n",
       "      <td>0.52%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>24.775</td>\n",
       "      <td>25.010</td>\n",
       "      <td>25.010</td>\n",
       "      <td>24.355</td>\n",
       "      <td>0.30K</td>\n",
       "      <td>-0.62%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-11</td>\n",
       "      <td>24.930</td>\n",
       "      <td>25.970</td>\n",
       "      <td>26.465</td>\n",
       "      <td>24.760</td>\n",
       "      <td>0.45K</td>\n",
       "      <td>-5.53%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.44%</td>\n",
       "      <td>34.299402</td>\n",
       "      <td>-0.128800</td>\n",
       "      <td>19.074238</td>\n",
       "      <td>19.420546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>2017-10-26</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.22%</td>\n",
       "      <td>33.531057</td>\n",
       "      <td>-0.117551</td>\n",
       "      <td>18.978597</td>\n",
       "      <td>19.367584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84%</td>\n",
       "      <td>34.323259</td>\n",
       "      <td>-0.098296</td>\n",
       "      <td>18.895873</td>\n",
       "      <td>19.318267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.72%</td>\n",
       "      <td>32.747079</td>\n",
       "      <td>-0.086934</td>\n",
       "      <td>18.806742</td>\n",
       "      <td>19.265001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-78.97%</td>\n",
       "      <td>35.511086</td>\n",
       "      <td>-0.063048</td>\n",
       "      <td>18.738481</td>\n",
       "      <td>19.218923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1617 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Dernier    Ouv.  Plus Haut Plus Bas   Vol. Variation %  \\\n",
       "0    2024-03-15   27.050  26.255     27.085   26.000  0.08K       3.84%   \n",
       "1    2024-03-14   26.050  24.675     26.600   24.675  0.51K       4.60%   \n",
       "2    2024-03-13   24.905  25.100     25.425   24.555  0.24K       0.52%   \n",
       "3    2024-03-12   24.775  25.010     25.010   24.355  0.30K      -0.62%   \n",
       "4    2024-03-11   24.930  25.970     26.465   24.760  0.45K      -5.53%   \n",
       "...         ...      ...     ...        ...      ...    ...         ...   \n",
       "1612 2017-10-27   18.150  18.150     18.150   18.150    NaN       0.44%   \n",
       "1613 2017-10-26   18.070  18.070     18.070   18.070    NaN      -0.22%   \n",
       "1614 2017-10-25   18.110  18.110     18.110   18.110    NaN       0.84%   \n",
       "1615 2017-10-24   17.960  17.960     17.960   17.960    NaN      -0.72%   \n",
       "1616 2017-10-23   18.090  18.090     18.090   18.090    NaN     -78.97%   \n",
       "\n",
       "            rsi      macd      ema20      ema50  \n",
       "0           NaN       NaN        NaN        NaN  \n",
       "1           NaN       NaN        NaN        NaN  \n",
       "2           NaN       NaN        NaN        NaN  \n",
       "3           NaN       NaN        NaN        NaN  \n",
       "4           NaN       NaN        NaN        NaN  \n",
       "...         ...       ...        ...        ...  \n",
       "1612  34.299402 -0.128800  19.074238  19.420546  \n",
       "1613  33.531057 -0.117551  18.978597  19.367584  \n",
       "1614  34.323259 -0.098296  18.895873  19.318267  \n",
       "1615  32.747079 -0.086934  18.806742  19.265001  \n",
       "1616  35.511086 -0.063048  18.738481  19.218923  \n",
       "\n",
       "[1617 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz['Dernier'] = pd.to_numeric(gaz['Dernier'], errors='coerce')\n",
    "\n",
    "gaz['rsi'] = ta.momentum.rsi(gaz['Dernier'], window=14)\n",
    "gaz['macd'] = ta.trend.macd_diff(gaz['Dernier'])\n",
    "gaz['ema20'] = ta.trend.ema_indicator(gaz['Dernier'], window=20)\n",
    "gaz['ema50'] = ta.trend.ema_indicator(gaz['Dernier'], window=50)\n",
    "\n",
    "gaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_k_m_to_numeric(value):\n",
    "    \"\"\"\n",
    "    Convert values with 'K' or 'M' suffix to float numbers.\n",
    "    Args:\n",
    "    - value: The string or numeric value to convert.\n",
    "    \n",
    "    Returns:\n",
    "    - The converted value as float if 'K' or 'M' was found; otherwise, the original value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):  # Only process strings\n",
    "        if value.endswith('K'):\n",
    "            return float(value[:-1]) * 1e3\n",
    "        elif value.endswith('M'):\n",
    "            return float(value[:-1]) * 1e6\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/qtyfs34j79n27bt7y8vw2n140000gn/T/ipykernel_2282/3463237170.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  assert gaz.applymap(np.isreal).all().all(), \"Non-numeric data found in the dataset.\"\n"
     ]
    }
   ],
   "source": [
    "gaz['Vol.'] = gaz['Vol.'].apply(convert_k_m_to_numeric)\n",
    "for column in gaz.columns:\n",
    "    gaz[column] = pd.to_numeric(gaz[column], errors='coerce')\n",
    "assert gaz.applymap(np.isreal).all().all(), \"Non-numeric data found in the dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def calculate_technical_indicators(self):\n",
    "        \"\"\"\n",
    "        Calculate specified technical indicators for the dataset.\n",
    "        \"\"\"\n",
    "        self.data['rsi'] = ta.momentum.rsi(self.data['Dernier'], window=14)\n",
    "        self.data['macd'] = ta.trend.macd_diff(self.data['Dernier'])\n",
    "        self.data['ema20'] = ta.trend.ema_indicator(self.data['Dernier'], window=20)\n",
    "        self.data['ema50'] = ta.trend.ema_indicator(self.data['Dernier'], window=50)\n",
    "\n",
    "    def normalize_features(self):\n",
    "        \"\"\"\n",
    "        Normalize features to have a similar scale.\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        features = ['rsi', 'macd', 'ema20', 'ema50']  # Specify the features to normalize\n",
    "        self.data[features] = scaler.fit_transform(self.data[features])\n",
    "\n",
    "    def integrate_economic_indicators(self, economic_data):\n",
    "        \"\"\"\n",
    "        Integrate economic indicators with market data.\n",
    "        This function assumes economic_data is a DataFrame where columns are indicators and rows align with self.data's timeline.\n",
    "        \"\"\"\n",
    "        self.data = pd.concat([self.data, economic_data], axis=1)\n",
    "\n",
    "    def construct_feature_vector(self):\n",
    "        \"\"\"\n",
    "        Combine all features into a single vector for each timestep.\n",
    "        Assuming all necessary features are already columns in self.data,\n",
    "        this function will return a numpy array representation of the DataFrame.\n",
    "        \"\"\"\n",
    "        return self.data.values\n",
    "\n",
    "feature_engineer = FeatureEngineer(gaz)\n",
    "\n",
    "feature_engineer.calculate_technical_indicators()  # Calculate RSI, MACD, EMA20, EMA50\n",
    "\n",
    "feature_engineer.normalize_features()  # Normalize the features\n",
    "features_vector = feature_engineer.construct_feature_vector()  # Get the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=10000, transaction_cost=0.001):\n",
    "        self.data = data\n",
    "        self.state_space = data.shape[1]  # This should match the number of features used to represent a state\n",
    "        self.action_space = 3  # For example: buy, sell,\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.done = False\n",
    "        self.position = 0\n",
    "        self.history = []  # To store trade history\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return self.data.iloc[self.current_step]\n",
    "\n",
    "    def step(self, action):        \n",
    "        # Ensure action is within a valid range\n",
    "        action = np.clip(action, -1, 1)\n",
    "\n",
    "        # Calculate the number of shares bought/sold based on the action\n",
    "        delta_position = action * self.balance  # This assumes all-in on each action\n",
    "\n",
    "        # Get the current price from the dataset to calculate changes in portfolio value\n",
    "        current_price = self.data.iloc[self.current_step]['Dernier']  # Assuming 'close' is a column in your dataset\n",
    "        next_step = min(self.current_step + 1, len(self.data) - 1)  # Ensure we don't go past the end of the dataset\n",
    "        next_price = self.data.iloc[next_step]['Dernier']\n",
    "\n",
    "        # Update position and balance\n",
    "        change_in_value = delta_position * (next_price - current_price) / current_price\n",
    "        self.balance += change_in_value - (abs(delta_position) * self.transaction_cost)\n",
    "        self.portfolio_value = self.balance  # This could be more complex if managing multiple positions\n",
    "        self.position += delta_position\n",
    "\n",
    "        self.current_step = next_step\n",
    "\n",
    "        # Check if we're at the end\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        # Here, the reward could be the change in portfolio value, or some other metric\n",
    "        reward = change_in_value - (abs(delta_position) * self.transaction_cost)\n",
    "        \n",
    "        # Record this step\n",
    "        self.history.append((self.current_step, self.position, self.portfolio_value, reward))\n",
    "\n",
    "        return self._next_observation(), reward, self.done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Step:\", self.current_step)\n",
    "        print(\"Balance:\", self.balance)\n",
    "        print(\"Position:\", self.position)\n",
    "        print(\"Portfolio Value:\", self.portfolio_value)\n",
    "\n",
    "    def run_backtest(self, policy):\n",
    "        \"\"\"\n",
    "        Run a full backtest of the trading environment using the provided policy.\n",
    "        The policy function should take a state as input and return an action.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        while not self.done:\n",
    "            current_state = self._next_observation()\n",
    "            action = policy(current_state)\n",
    "            self.step(action)\n",
    "        return self.history\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Define a simple policy function (replace this with your PPO agent's policy for real testing)\n",
    "def sample_policy(state):\n",
    "    # This is a dummy policy that randomly decides to buy, hold, or sell\n",
    "    return np.random.uniform(-1, 1)\n",
    "\n",
    "# Assuming 'gaz_data' is your preprocessed DataFrame including 'Dernier' prices and any features\n",
    "env = TradingEnvironment(gaz)\n",
    "backtest_history = env.run_backtest(sample_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "import numpy as np\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, action_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.action_bound = action_bound\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.action_bound * self.network(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size + action_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_size, action_size, action_bound, lr_actor=1e-4, lr_critic=2e-4):\n",
    "        self.actor = Actor(state_size, action_size, action_bound)\n",
    "        self.critic = Critic(state_size, action_size)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        # Your preprocessing logic here, making sure the state is a float tensor\n",
    "        return torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        state = self.preprocess_state(state)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * next_value * masks[step] - values[step]\n",
    "            gae = delta + gamma * tau * masks[step] * gae\n",
    "            next_value = values[step]\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update_policy(self, states, actions, rewards, next_states, dones):\n",
    "        advantages, discounted_rewards = self.calculate_advantages(rewards, states, next_states, dones)\n",
    "        \n",
    "        # Convert lists to numpy arrays for processing\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        advantages = np.array(advantages)\n",
    "        discounted_rewards = np.array(discounted_rewards)\n",
    "\n",
    "        # Update actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Calculate loss for the actor\n",
    "            actor_loss = self.calculate_actor_loss(states, actions, advantages)\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "\n",
    "        # Update critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Calculate loss for the critic\n",
    "            critic_loss = self.calculate_critic_loss(states, discounted_rewards)\n",
    "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "\n",
    "    def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * next_value * masks[step] - values[step]\n",
    "            gae = delta + gamma * tau * masks[step] * gae\n",
    "            next_value = values[step]\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "# Define the environment and PPO agent parameters\n",
    "state_size = 100  # Assume 100 features for the state\n",
    "action_size = 1  # Buy/sell quantity as a single action\n",
    "action_bound = 1  # Action limit (e.g., normalized quantity between -1 and 1)\n",
    "\n",
    "# Instantiate the PPOAgent\n",
    "agent = PPOAgent(state_size=env.state_space, action_size=env.action_space, action_bound=1)\n",
    "\n",
    "# Now you can access actor and critic through the agent object\n",
    "actor_model = Actor(state_size=100, action_size=1, action_bound=1)\n",
    "critic_model = Critic(state_size=100, action_size=1)\n",
    "\n",
    "summary(actor_model, input_size=(1, 100))\n",
    "summary(critic_model, input_size=(1, 101)) \n",
    "critic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.predict_action(state)  # Predict the action for the current state\n",
    "            next_state, reward, done, _ = env.step(action)  # Take the action in the environment\n",
    "            agent.update_policy(state, action, reward, next_state, done)  # Update the policy\n",
    "            state = next_state\n",
    "        print(f\"Episode {episode + 1}: Complete\")\n",
    "    print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_pnl = 0\n",
    "    print(\"Date\\t\\tAction\\tReward\\tPortfolio Value\")\n",
    "\n",
    "    while not done:\n",
    "        action = agent.predict_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Action interpretation for logging: -1 (Sell), 1 (Buy), 0 (Hold)\n",
    "        action_str = \"Buy\" if action > 0 else \"Sell\" if action < 0 else \"Hold\"\n",
    "\n",
    "        # Assuming 'date' is part of the environment's state\n",
    "        date = state.index[env.current_step].strftime('%Y-%m-%d')\n",
    "        print(f\"{date}\\t{action_str}\\t{reward:.2f}\\t{env.portfolio_value:.2f}\")\n",
    "\n",
    "        total_pnl += reward\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Final PnL: {total_pnl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(gaz)  # Ensure 'gaz' is your DataFrame properly prepared\n",
    "agent = PPOAgent(state_size=env.state_space, action_size=env.action_space, action_bound=1)  # Adjust parameters as needed\n",
    "\n",
    "# Proceed with training and running the simulation\n",
    "train_agent(env, agent)\n",
    "run_simulation(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAUDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=10000, transaction_cost=0.001):\n",
    "        self.data = data\n",
    "        self.state_space = data.shape[1] # This should match the number of features used to represent a state\n",
    "        self.action_space = 3 # For example: buy, sell, hold\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.done = False\n",
    "        self.position = 0\n",
    "        self.history = [] # To store trade history\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return self.data.iloc[self.current_step]\n",
    "\n",
    "    def step(self, action):\n",
    "        # Ensure action is within a valid range\n",
    "        action = np.clip(action, -1, 1)\n",
    "        # Calculate the number of shares bought/sold based on the action\n",
    "        delta_position = action * self.balance # This assumes all-in on each action\n",
    "        # Get the current price from the dataset to calculate changes in portfolio value\n",
    "        current_price = self.data.iloc[self.current_step]['Dernier'] # Assuming 'close' is a column in your dataset\n",
    "        next_step = min(self.current_step + 1, len(self.data) - 1) # Ensure we don't go past the end of the dataset\n",
    "        next_price = self.data.iloc[next_step]['Dernier']\n",
    "        # Update position and balance\n",
    "        change_in_value = delta_position * (next_price - current_price) / current_price\n",
    "        self.balance += change_in_value - (abs(delta_position) * self.transaction_cost)\n",
    "        self.portfolio_value = self.balance # This could be more complex if managing multiple positions\n",
    "        self.position += delta_position\n",
    "        self.current_step = next_step\n",
    "        # Check if we're at the end\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "        # Here, the reward could be the change in portfolio value, or some other metric\n",
    "        reward = change_in_value - (abs(delta_position) * self.transaction_cost)\n",
    "        # Record this step\n",
    "        self.history.append((self.current_step, self.position, self.portfolio_value, reward))\n",
    "        return self._next_observation(), reward, self.done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Step:\", self.current_step)\n",
    "        print(\"Balance:\", self.balance)\n",
    "        print(\"Position:\", self.position)\n",
    "        print(\"Portfolio Value:\", self.portfolio_value)\n",
    "\n",
    "    def run_backtest(self, policy):\n",
    "        \"\"\"\n",
    "        Run a full backtest of the trading environment using the provided policy.\n",
    "        The policy function should take a state as input and return an action.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        while not self.done:\n",
    "            current_state = self._next_observation()\n",
    "            action = policy(current_state)\n",
    "            self.step(action)\n",
    "        return self.history\n",
    "\n",
    "# Example usage:\n",
    "# Define a simple policy function (replace this with your PPO agent's policy for real testing)\n",
    "def sample_policy(state):\n",
    "    # This is a dummy policy that randomly decides to buy, hold, or sell\n",
    "    return np.random.uniform(-1, 1)\n",
    "\n",
    "# Assuming 'gaz_data' is your preprocessed DataFrame including 'Dernier' prices and any features\n",
    "env = TradingEnvironment(gaz)\n",
    "backtest_history = env.run_backtest(sample_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, action_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.action_bound = action_bound\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.action_bound * self.network(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size + action_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Critic.__init__() missing 1 required positional argument: 'action_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb Cell 18\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb#X25sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m action_bound \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Action limit (e.g., normalized quantity between -1 and 1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb#X25sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Instantiate the PPOAgent\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb#X25sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m agent \u001b[39m=\u001b[39m PPOAgent(state_size\u001b[39m=\u001b[39;49mstate_size, action_size\u001b[39m=\u001b[39;49maction_size, action_bound\u001b[39m=\u001b[39;49maction_bound)\n",
      "\u001b[1;32m/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, state_size, action_size, action_bound, lr_actor\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m, lr_critic\u001b[39m=\u001b[39m\u001b[39m2e-4\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor \u001b[39m=\u001b[39m Actor(state_size, action_size, action_bound)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic \u001b[39m=\u001b[39m Critic(state_size)  \u001b[39m# Critic takes only state as input\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_actor \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr_actor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b23/Documents/GitHub/Reinforcement-Learning-Project/RL.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_critic \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr_critic)\n",
      "\u001b[0;31mTypeError\u001b[0m: Critic.__init__() missing 1 required positional argument: 'action_size'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the necessary imports and definitions for Actor and Critic classes\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_size, action_size, action_bound, lr_actor=1e-4, lr_critic=2e-4):\n",
    "        self.actor = Actor(state_size, action_size, action_bound)\n",
    "        self.critic = Critic(state_size)  # Critic takes only state as input\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        # Your preprocessing logic here, making sure the state is a float tensor\n",
    "        return torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "    def predict_action(self, state):\n",
    "        state = self.preprocess_state(state)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def compute_gae(self, next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * next_value * masks[step] - values[step]\n",
    "            gae = delta + gamma * tau * masks[step] * gae\n",
    "            next_value = values[step]\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def update_policy(self, states, actions, rewards, next_states, dones):\n",
    "        advantages, discounted_rewards = self.calculate_advantages(rewards, states, next_states, dones)\n",
    "        # Convert lists to tensors for processing\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        advantages = torch.FloatTensor(advantages)\n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards)\n",
    "\n",
    "        # Update actor\n",
    "        actor_loss = self.calculate_actor_loss(states, actions, advantages)\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        # Update critic\n",
    "        critic_loss = self.calculate_critic_loss(states, discounted_rewards)\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "# Define the environment and PPO agent parameters\n",
    "state_size = 100  # Assume 100 features for the state\n",
    "action_size = 1  # Buy/sell quantity as a single action\n",
    "action_bound = 1  # Action limit (e.g., normalized quantity between -1 and 1)\n",
    "\n",
    "# Instantiate the PPOAgent\n",
    "agent = PPOAgent(state_size=state_size, action_size=action_size, action_bound=action_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.predict_action(state)  # Predict the action for the current state\n",
    "            next_state, reward, done, _ = env.step(action)  # Take the action in the environment\n",
    "            agent.update_policy(state, action, reward, next_state, done)  # Update the policy\n",
    "            state = next_state\n",
    "        print(f\"Episode {episode + 1}: Complete\")\n",
    "    print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_pnl = 0\n",
    "    print(\"Date\\t\\tAction\\tReward\\tPortfolio Value\")\n",
    "    while not done:\n",
    "        action = agent.predict_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # Action interpretation for logging: -1 (Sell), 1 (Buy), 0 (Hold)\n",
    "        action_str = \"Buy\" if action > 0 else \"Sell\" if action < 0 else \"Hold\"\n",
    "        # Assuming 'date' is part of the environment's state\n",
    "        date = state.index[env.current_step].strftime('%Y-%m-%d')\n",
    "        print(f\"{date}\\t{action_str}\\t{reward:.2f}\\t{env.portfolio_value:.2f}\")\n",
    "        total_pnl += reward\n",
    "        state = next_state\n",
    "    print(f\"Final PnL: {total_pnl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(gaz)  \n",
    "agent = PPOAgent(state_size=state_size, action_size=action_size, action_bound=action_bound)\n",
    "\n",
    "# Proceed with training and running the simulation\n",
    "train_agent(env, agent)\n",
    "run_simulation(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
