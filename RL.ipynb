{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/p9wc5ctx14z50221phyqkyz80000gn/T/ipykernel_55714/1168937049.py:2: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  gaz['Date']=pd.to_datetime(gaz['Date'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Dernier', 'Ouv.', ' Plus Haut', 'Plus Bas', 'Vol.',\n",
       "       'Variation %'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz = pd.read_csv('Data/Dutch TTF Natural Gas Futures - Données Historiques (1).csv')\n",
    "gaz['Date']=pd.to_datetime(gaz['Date'])\n",
    "lists = ['Dernier', 'Ouv.', ' Plus Haut', 'Plus Bas', 'Vol.','Variation %']\n",
    "for i in lists: \n",
    "    gaz[i] = gaz[i].str.replace(',', '.')\n",
    "gaz.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Dernier</th>\n",
       "      <th>Ouv.</th>\n",
       "      <th>Plus Haut</th>\n",
       "      <th>Plus Bas</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Variation %</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>ema20</th>\n",
       "      <th>ema50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-15</td>\n",
       "      <td>27.050</td>\n",
       "      <td>26.255</td>\n",
       "      <td>27.085</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0.08K</td>\n",
       "      <td>3.84%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-14</td>\n",
       "      <td>26.050</td>\n",
       "      <td>24.675</td>\n",
       "      <td>26.600</td>\n",
       "      <td>24.675</td>\n",
       "      <td>0.51K</td>\n",
       "      <td>4.60%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-13</td>\n",
       "      <td>24.905</td>\n",
       "      <td>25.100</td>\n",
       "      <td>25.425</td>\n",
       "      <td>24.555</td>\n",
       "      <td>0.24K</td>\n",
       "      <td>0.52%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>24.775</td>\n",
       "      <td>25.010</td>\n",
       "      <td>25.010</td>\n",
       "      <td>24.355</td>\n",
       "      <td>0.30K</td>\n",
       "      <td>-0.62%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-11</td>\n",
       "      <td>24.930</td>\n",
       "      <td>25.970</td>\n",
       "      <td>26.465</td>\n",
       "      <td>24.760</td>\n",
       "      <td>0.45K</td>\n",
       "      <td>-5.53%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>18.150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.44%</td>\n",
       "      <td>34.299402</td>\n",
       "      <td>-0.128800</td>\n",
       "      <td>19.074238</td>\n",
       "      <td>19.420546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>2017-10-26</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>18.070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.22%</td>\n",
       "      <td>33.531057</td>\n",
       "      <td>-0.117551</td>\n",
       "      <td>18.978597</td>\n",
       "      <td>19.367584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>18.110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84%</td>\n",
       "      <td>34.323259</td>\n",
       "      <td>-0.098296</td>\n",
       "      <td>18.895873</td>\n",
       "      <td>19.318267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>17.960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.72%</td>\n",
       "      <td>32.747079</td>\n",
       "      <td>-0.086934</td>\n",
       "      <td>18.806742</td>\n",
       "      <td>19.265001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>18.090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-78.97%</td>\n",
       "      <td>35.511086</td>\n",
       "      <td>-0.063048</td>\n",
       "      <td>18.738481</td>\n",
       "      <td>19.218923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1617 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Dernier    Ouv.  Plus Haut Plus Bas   Vol. Variation %  \\\n",
       "0    2024-03-15   27.050  26.255     27.085   26.000  0.08K       3.84%   \n",
       "1    2024-03-14   26.050  24.675     26.600   24.675  0.51K       4.60%   \n",
       "2    2024-03-13   24.905  25.100     25.425   24.555  0.24K       0.52%   \n",
       "3    2024-03-12   24.775  25.010     25.010   24.355  0.30K      -0.62%   \n",
       "4    2024-03-11   24.930  25.970     26.465   24.760  0.45K      -5.53%   \n",
       "...         ...      ...     ...        ...      ...    ...         ...   \n",
       "1612 2017-10-27   18.150  18.150     18.150   18.150    NaN       0.44%   \n",
       "1613 2017-10-26   18.070  18.070     18.070   18.070    NaN      -0.22%   \n",
       "1614 2017-10-25   18.110  18.110     18.110   18.110    NaN       0.84%   \n",
       "1615 2017-10-24   17.960  17.960     17.960   17.960    NaN      -0.72%   \n",
       "1616 2017-10-23   18.090  18.090     18.090   18.090    NaN     -78.97%   \n",
       "\n",
       "            rsi      macd      ema20      ema50  \n",
       "0           NaN       NaN        NaN        NaN  \n",
       "1           NaN       NaN        NaN        NaN  \n",
       "2           NaN       NaN        NaN        NaN  \n",
       "3           NaN       NaN        NaN        NaN  \n",
       "4           NaN       NaN        NaN        NaN  \n",
       "...         ...       ...        ...        ...  \n",
       "1612  34.299402 -0.128800  19.074238  19.420546  \n",
       "1613  33.531057 -0.117551  18.978597  19.367584  \n",
       "1614  34.323259 -0.098296  18.895873  19.318267  \n",
       "1615  32.747079 -0.086934  18.806742  19.265001  \n",
       "1616  35.511086 -0.063048  18.738481  19.218923  \n",
       "\n",
       "[1617 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaz['Dernier'] = pd.to_numeric(gaz['Dernier'], errors='coerce')\n",
    "\n",
    "gaz['rsi'] = ta.momentum.rsi(gaz['Dernier'], window=14)\n",
    "gaz['macd'] = ta.trend.macd_diff(gaz['Dernier'])\n",
    "gaz['ema20'] = ta.trend.ema_indicator(gaz['Dernier'], window=20)\n",
    "gaz['ema50'] = ta.trend.ema_indicator(gaz['Dernier'], window=50)\n",
    "\n",
    "gaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_k_m_to_numeric(value):\n",
    "    \"\"\"\n",
    "    Convert values with 'K' or 'M' suffix to float numbers.\n",
    "    Args:\n",
    "    - value: The string or numeric value to convert.\n",
    "    \n",
    "    Returns:\n",
    "    - The converted value as float if 'K' or 'M' was found; otherwise, the original value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):  # Only process strings\n",
    "        if value.endswith('K'):\n",
    "            return float(value[:-1]) * 1e3\n",
    "        elif value.endswith('M'):\n",
    "            return float(value[:-1]) * 1e6\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/p9wc5ctx14z50221phyqkyz80000gn/T/ipykernel_55714/3463237170.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  assert gaz.applymap(np.isreal).all().all(), \"Non-numeric data found in the dataset.\"\n"
     ]
    }
   ],
   "source": [
    "gaz['Vol.'] = gaz['Vol.'].apply(convert_k_m_to_numeric)\n",
    "for column in gaz.columns:\n",
    "    gaz[column] = pd.to_numeric(gaz[column], errors='coerce')\n",
    "assert gaz.applymap(np.isreal).all().all(), \"Non-numeric data found in the dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def calculate_technical_indicators(self):\n",
    "        \"\"\"\n",
    "        Calculate specified technical indicators for the dataset.\n",
    "        \"\"\"\n",
    "        self.data['rsi'] = ta.momentum.rsi(self.data['Dernier'], window=14)\n",
    "        self.data['macd'] = ta.trend.macd_diff(self.data['Dernier'])\n",
    "        self.data['ema20'] = ta.trend.ema_indicator(self.data['Dernier'], window=20)\n",
    "        self.data['ema50'] = ta.trend.ema_indicator(self.data['Dernier'], window=50)\n",
    "\n",
    "    def normalize_features(self):\n",
    "        \"\"\"\n",
    "        Normalize features to have a similar scale.\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        features = ['rsi', 'macd', 'ema20', 'ema50']  # Specify the features to normalize\n",
    "        self.data[features] = scaler.fit_transform(self.data[features])\n",
    "\n",
    "    def integrate_economic_indicators(self, economic_data):\n",
    "        \"\"\"\n",
    "        Integrate economic indicators with market data.\n",
    "        This function assumes economic_data is a DataFrame where columns are indicators and rows align with self.data's timeline.\n",
    "        \"\"\"\n",
    "        self.data = pd.concat([self.data, economic_data], axis=1)\n",
    "\n",
    "    def construct_feature_vector(self):\n",
    "        \"\"\"\n",
    "        Combine all features into a single vector for each timestep.\n",
    "        Assuming all necessary features are already columns in self.data,\n",
    "        this function will return a numpy array representation of the DataFrame.\n",
    "        \"\"\"\n",
    "        return self.data.values\n",
    "\n",
    "feature_engineer = FeatureEngineer(gaz)\n",
    "\n",
    "feature_engineer.calculate_technical_indicators()  # Calculate RSI, MACD, EMA20, EMA50\n",
    "\n",
    "feature_engineer.normalize_features()  # Normalize the features\n",
    "features_vector = feature_engineer.construct_feature_vector()  # Get the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Dernier</th>\n",
       "      <th>Ouv.</th>\n",
       "      <th>Plus Haut</th>\n",
       "      <th>Plus Bas</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Variation %</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>ema20</th>\n",
       "      <th>ema50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1710460800000000000</td>\n",
       "      <td>27.050</td>\n",
       "      <td>26.255</td>\n",
       "      <td>27.085</td>\n",
       "      <td>26.000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1710374400000000000</td>\n",
       "      <td>26.050</td>\n",
       "      <td>24.675</td>\n",
       "      <td>26.600</td>\n",
       "      <td>24.675</td>\n",
       "      <td>510.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1710288000000000000</td>\n",
       "      <td>24.905</td>\n",
       "      <td>25.100</td>\n",
       "      <td>25.425</td>\n",
       "      <td>24.555</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1710201600000000000</td>\n",
       "      <td>24.775</td>\n",
       "      <td>25.010</td>\n",
       "      <td>25.010</td>\n",
       "      <td>24.355</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1710115200000000000</td>\n",
       "      <td>24.930</td>\n",
       "      <td>25.970</td>\n",
       "      <td>26.465</td>\n",
       "      <td>24.760</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date  Dernier    Ouv.   Plus Haut  Plus Bas   Vol.  \\\n",
       "0  1710460800000000000   27.050  26.255      27.085    26.000   80.0   \n",
       "1  1710374400000000000   26.050  24.675      26.600    24.675  510.0   \n",
       "2  1710288000000000000   24.905  25.100      25.425    24.555  240.0   \n",
       "3  1710201600000000000   24.775  25.010      25.010    24.355  300.0   \n",
       "4  1710115200000000000   24.930  25.970      26.465    24.760  450.0   \n",
       "\n",
       "   Variation %  rsi  macd  ema20  ema50  \n",
       "0          0.0  0.0   0.0    0.0    0.0  \n",
       "1          0.0  0.0   0.0    0.0    0.0  \n",
       "2          0.0  0.0   0.0    0.0    0.0  \n",
       "3          0.0  0.0   0.0    0.0    0.0  \n",
       "4          0.0  0.0   0.0    0.0    0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a copy of gaz and fill NaN values with 0\n",
    "gaz_copy = gaz.copy()\n",
    "gaz_copy.fillna(0, inplace=True)\n",
    "\n",
    "gaz_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=10000, transaction_cost=0.001):\n",
    "        self.data = data\n",
    "        self.state_space = data.shape[1]  # This should match the number of features used to represent a state\n",
    "        self.action_space = 3  # For example: buy, sell,\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.done = False\n",
    "        self.position = 0\n",
    "        self.history = []  # To store trade history\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return self.data.iloc[self.current_step]\n",
    "\n",
    "    def step(self, action):        \n",
    "        # Ensure action is within a valid range\n",
    "        action = np.clip(action, -1, 1)\n",
    "\n",
    "        # Calculate the number of shares bought/sold based on the action\n",
    "        delta_position = action * self.balance  # This assumes all-in on each action\n",
    "\n",
    "        # Get the current price from the dataset to calculate changes in portfolio value\n",
    "        current_price = self.data.iloc[self.current_step]['Dernier']  # Assuming 'close' is a column in your dataset\n",
    "        next_step = min(self.current_step + 1, len(self.data) - 1)  # Ensure we don't go past the end of the dataset\n",
    "        next_price = self.data.iloc[next_step]['Dernier']\n",
    "\n",
    "        # Update position and balance\n",
    "        change_in_value = delta_position * (next_price - current_price) / current_price\n",
    "        self.balance += change_in_value - (abs(delta_position) * self.transaction_cost)\n",
    "        self.portfolio_value = self.balance  # This could be more complex if managing multiple positions\n",
    "        self.position += delta_position\n",
    "\n",
    "        self.current_step = next_step\n",
    "\n",
    "        # Check if we're at the end\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        # Here, the reward could be the change in portfolio value, or some other metric\n",
    "        reward = change_in_value - (abs(delta_position) * self.transaction_cost)\n",
    "        \n",
    "        # Record this step\n",
    "        self.history.append((self.current_step, self.position, self.portfolio_value, reward))\n",
    "\n",
    "        return self._next_observation(), reward, self.done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Step:\", self.current_step)\n",
    "        print(\"Balance:\", self.balance)\n",
    "        print(\"Position:\", self.position)\n",
    "        print(\"Portfolio Value:\", self.portfolio_value)\n",
    "\n",
    "    def run_backtest(self, policy):\n",
    "        \"\"\"\n",
    "        Run a full backtest of the trading environment using the provided policy.\n",
    "        The policy function should take a state as input and return an action.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        while not self.done:\n",
    "            current_state = self._next_observation()\n",
    "            action = policy(current_state)\n",
    "            self.step(action)\n",
    "        return self.history\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Define a simple policy function (replace this with your PPO agent's policy for real testing)\n",
    "def sample_policy(state):\n",
    "    # This is a dummy policy that randomly decides to buy, hold, or sell\n",
    "    return np.random.uniform(-1, 1)\n",
    "\n",
    "# Assuming 'gaz_data' is your preprocessed DataFrame including 'Dernier' prices and any features\n",
    "env = TradingEnvironment(gaz)\n",
    "backtest_history = env.run_backtest(sample_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 16:39:58.172451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "import numpy as np\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, action_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.action_bound = action_bound\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.action_bound * self.network(state)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output is a single value representing V(s)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_size, action_size, action_bound, lr_actor=1e-4, lr_critic=1e-3, action_std=0.5 , update_epochs=10 , clip_param=0.2 , entropy_beta=0.01):\n",
    "        self.actor = Actor(state_size, action_size, action_bound)\n",
    "        self.critic = Critic(state_size)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.action_std = action_std  # Define the standard deviation for action distribution\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.95\n",
    "        self.update_epochs = update_epochs\n",
    "        self.clip_param = clip_param\n",
    "        self.entropy_beta = entropy_beta\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "    def predict_action(self, state, return_log_prob=False):\n",
    "        state = self.preprocess_state(state)\n",
    "        with torch.no_grad():\n",
    "            action_mean = self.actor(state)\n",
    "            dist = torch.distributions.Normal(action_mean, self.action_std)  # Assuming self.action_std is defined\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action) if return_log_prob else None\n",
    "        return action.cpu().numpy(), log_prob\n",
    "\n",
    "\n",
    "    def compute_gae(self, next_value, rewards, masks, values):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * next_value * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * self.tau * masks[step] * gae\n",
    "            next_value = values[step]\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "\n",
    "    def update_policy(self, states, actions, rewards, next_states, dones, old_log_probs):\n",
    "        # Normalize rewards\n",
    "        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-10)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        old_log_probs = torch.stack(old_log_probs)\n",
    "        \n",
    "\n",
    "        for _ in range(self.update_epochs):\n",
    "            log_probs, state_values, dist_entropy = self.evaluate_actions(states, actions)\n",
    "            advantages, returns = self.calculate_advantages(rewards, states, next_states, dones, state_values)\n",
    "\n",
    "            # Calculate actor loss\n",
    "            # print(\"log_probs.shape:\", log_probs.shape)\n",
    "            old_log_probs = old_log_probs.squeeze()\n",
    "            old_log_probs = old_log_probs.mean(dim=-1).squeeze()\n",
    "            # print(\"old_log_probs.shape:\", old_log_probs.shape)\n",
    "            ratio = torch.exp(log_probs - old_log_probs.detach())\n",
    "\n",
    "            # print(\"ratio.shape:\", ratio.shape)\n",
    "            # print(\"advantages.shape:\", advantages.shape)\n",
    "\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - self.entropy_beta * dist_entropy.mean()\n",
    "\n",
    "            # Calculate critic loss\n",
    "            critic_loss = (returns - state_values).pow(2).mean()\n",
    "\n",
    "            # Perform backprop\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
    "            self.optimizer_actor.step()\n",
    "\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
    "            self.optimizer_critic.step()\n",
    "\n",
    "    def calculate_advantages(self, rewards, states, next_states, dones, state_values):\n",
    "        with torch.no_grad():\n",
    "            aggregated_rewards = rewards.mean(dim=-1).squeeze()\n",
    "            next_state_values = self.critic(next_states).squeeze()\n",
    "            \n",
    "\n",
    "            # print(\"Before deltas computation\")\n",
    "\n",
    "            state_values = state_values.squeeze()\n",
    "\n",
    "            # print(\"aggregated_rewards.shape:\", aggregated_rewards.shape)\n",
    "            # print(\"next_state_values.shape:\", next_state_values.shape)\n",
    "            # print(\"dones.shape:\", dones.shape)\n",
    "            # print(\"state_values.shape:\", state_values.shape)\n",
    "            \n",
    "\n",
    "            deltas = aggregated_rewards + self.gamma * next_state_values * (1 - dones) - state_values\n",
    "\n",
    "\n",
    "            # print(\"After deltas computation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            advantages = torch.zeros_like(aggregated_rewards)\n",
    "            running_add = 0.0\n",
    "            for t in reversed(range(len(aggregated_rewards))):\n",
    "                # print(\"deltas[t].shape:\", deltas[t].shape)\n",
    "                # print(\"self.gamma:\", self.gamma)\n",
    "                # print(\"self.tau:\", self.tau)\n",
    "                # print(\"dones[t].shape:\", dones[t].shape)\n",
    "                # print(\"running_add:\", running_add)\n",
    "                # print(\"advantages[t].shape:\", advantages[t].shape)\n",
    "\n",
    "                running_add = deltas[t] + self.gamma * self.tau * (1 - dones[t]) * running_add\n",
    "                \n",
    "                # print(\"running_add.shape after computation:\", running_add.shape)\n",
    "                advantages[t] = running_add\n",
    "\n",
    "            returns = advantages + state_values\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "            return advantages, returns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_actions(self, states, actions):\n",
    "        states_np = np.array(states)  # Convert list to numpy array\n",
    "        states = torch.FloatTensor(states_np)  # Then convert to tensor\n",
    "\n",
    "        actions_np = np.array(actions)  # Convert list to numpy array\n",
    "        actions = torch.FloatTensor(actions_np).squeeze(1)  # Then convert to tensor\n",
    "        # actions = torch.FloatTensor(actions).squeeze(1)\n",
    "\n",
    "        action_means = self.actor(states)  # Expected shape: [1616, 3]\n",
    "        state_values = self.critic(states).squeeze(-1)  # Ensure [1616]\n",
    "        \n",
    "        dist = torch.distributions.Normal(action_means, self.action_std)\n",
    "\n",
    "        # Calculate log probabilities and ensure it's reduced to [1616]\n",
    "        # print(\"actions.shape:\", actions.shape)\n",
    "        log_probs = dist.log_prob(actions).sum(dim=-1)  # Summing over action dimensions\n",
    "\n",
    "        # Calculate entropy and ensure it's already correctly shaped [1616]\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "\n",
    "        # print(\"log_probs.shape:\", log_probs.shape)  # Should be [1616]\n",
    "        # print(\"state_values.shape:\", state_values.shape)  # Should be [1616]\n",
    "        # print(\"entropy.shape:\", entropy.shape)  # Should be [1616]\n",
    "\n",
    "        return log_probs, state_values, entropy\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Define the environment and PPO agent parameters\n",
    "state_size = 100  # Assume 100 features for the state\n",
    "action_size = 1  # Buy/sell quantity as a single action\n",
    "action_bound = 1  # Action limit (e.g., normalized quantity between -1 and 1)\n",
    "\n",
    "# Instantiate the PPOAgent\n",
    "agent = PPOAgent(state_size=env.state_space, action_size=env.action_space, action_bound=1)\n",
    "\n",
    "# Now you can access actor and critic through the agent object\n",
    "actor_model = Actor(state_size=100, action_size=1, action_bound=1)\n",
    "critic_model = Critic(state_size=100)\n",
    "\n",
    "# summary(actor_model, input_size=(1, 100))\n",
    "# summary(critic_model, input_size=(1, 101))\n",
    "\n",
    "# critic_model.summary()\n",
    "print(actor_model)\n",
    "print(critic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_agent(env, agent, episodes=1000):\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for episode in tqdm(range(episodes)):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            states, actions, rewards, next_states, dones, old_log_probs = [], [], [], [], [], []\n",
    "            while not done:\n",
    "                action, log_prob = agent.predict_action(state, return_log_prob=True)  # Updated call\n",
    "\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                # Store experiences\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "                old_log_probs.append(log_prob)\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "            # After collecting experience, update the policy\n",
    "            agent.update_policy(states, actions, rewards, next_states, dones, old_log_probs)\n",
    "\n",
    "            # print total rewards for the episode\n",
    "            total_rewards = sum(sum(rewards)[0])\n",
    "            wandb.log({\"total_rewards\": total_rewards})\n",
    "\n",
    "            # print(f\"Episode {episode + 1}: Total Rewards: {total_rewards}\")\n",
    "\n",
    "        print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_pnl = 0\n",
    "    print(\"Date\\t\\tAction\\tReward\\tPortfolio Value\")\n",
    "\n",
    "    while not done:\n",
    "        action = agent.predict_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Action interpretation for logging: -1 (Sell), 1 (Buy), 0 (Hold)\n",
    "        action_str = \"Buy\" if action > 0 else \"Sell\" if action < 0 else \"Hold\"\n",
    "\n",
    "        # Assuming 'date' is part of the environment's state\n",
    "        date = state.index[env.current_step].strftime('%Y-%m-%d')\n",
    "        print(f\"{date}\\t{action_str}\\t{reward:.2f}\\t{env.portfolio_value:.2f}\")\n",
    "\n",
    "        total_pnl += reward\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "    print(f\"Final PnL: {total_pnl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpnikitits\u001b[0m (\u001b[33mpnikitits_1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/pierre/Documents/GitHub/Reinforcement-Learning-Project/wandb/run-20240323_164004-zc4120cd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pnikitits_1/RL%20Trading/runs/zc4120cd' target=\"_blank\">fresh-planet-5</a></strong> to <a href='https://wandb.ai/pnikitits_1/RL%20Trading' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pnikitits_1/RL%20Trading' target=\"_blank\">https://wandb.ai/pnikitits_1/RL%20Trading</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pnikitits_1/RL%20Trading/runs/zc4120cd' target=\"_blank\">https://wandb.ai/pnikitits_1/RL%20Trading/runs/zc4120cd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/var/folders/9z/p9wc5ctx14z50221phyqkyz80000gn/T/ipykernel_55714/2407987356.py:15: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.FloatTensor(state).unsqueeze(0)\n",
      "/var/folders/9z/p9wc5ctx14z50221phyqkyz80000gn/T/ipykernel_55714/2407987356.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  states = torch.FloatTensor(states)\n",
      "/var/folders/9z/p9wc5ctx14z50221phyqkyz80000gn/T/ipykernel_55714/2407987356.py:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  actions = torch.FloatTensor(actions)\n",
      "/var/folders/9z/p9wc5ctx14z50221phyqkyz80000gn/T/ipykernel_55714/2407987356.py:45: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  next_states = torch.FloatTensor(next_states)\n",
      "100%|██████████| 1000/1000 [24:09<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n",
      "Date\t\tAction\tReward\tPortfolio Value\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Proceed with training and running the simulation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m train_agent(env, agent)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m, in \u001b[0;36mrun_simulation\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      8\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpredict_action(state)\n\u001b[0;32m----> 9\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Action interpretation for logging: -1 (Sell), 1 (Buy), 0 (Hold)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     action_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSell\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHold\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mTradingEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):        \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Ensure action is within a valid range\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Calculate the number of shares bought/sold based on the action\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     delta_position \u001b[38;5;241m=\u001b[39m action \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbalance  \u001b[38;5;66;03m# This assumes all-in on each action\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/traderl/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2169\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2167\u001b[0m \n\u001b[1;32m   2168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/traderl/lib/python3.12/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/traderl/lib/python3.12/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"RL Trading\", config={\n",
    "    \"episodes\": 1000,\n",
    "    \"learning_rate_actor\": 1e-4,\n",
    "    \"learning_rate_critic\": 1e-3,\n",
    "    # Add other relevant configuration parameters here\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "\n",
    "env = TradingEnvironment(gaz_copy)\n",
    "agent = PPOAgent(state_size=env.state_space, action_size=env.action_space, action_bound=1)\n",
    "\n",
    "# Proceed with training and running the simulation\n",
    "train_agent(env, agent)\n",
    "run_simulation(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
